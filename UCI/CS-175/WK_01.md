# CS-175

## Week 1 Lecture 2

- Tokenization
- Vocabulary creation
    - Stopword removal, rare and common words, n-grams 
- “Bag of Words” representations
- Adding external information about words
    - Part of speech tagging, lexicons
- This functionality is already implemented in libraries such as NLTK, PyTorch

### Text Classification


#### Text Clasification Pipeline
(1) Input Text -> 2) Tokenize -> 3) Map to Vocabulary -> 4) Features -> 5) Classification Model -> 6) Class Probabilities  

1. Text Preprocessing
   1. Input Text
   2. Tokenize
        - Tokens:
          - Grouping of characters in a text string or document
          - individual words + possibly num, puncts, etc
        - Word TYpes or Terms
          - Unique tokens (or token combination) that are meaningful
            - Can include puncts and symbols
            - Can include bigrams (San Diego), trigrams (New York City), etc
        - Vocabulary
    3. Map to Vocabulary
    4. Features 
2. Machine Learning
   1. Features
   2. Classification Model
   3. Class Probabilities
        - Classification model will (usually) produce
output in the form of class probabilities

#### Concepts and Terminology
- Document
  - Sequence of tokens (words, etc)
    - A book, a news article,
  - the definition of a document is flexible
- Corpus: a collection of documents 
  -  e.g., all Yelp reviews for restaurants in Chicago

### Bag of Words BOW
- Represent a sentence or document by the counts of each word
- The sequence of words is ignored
    -  A word gets the same count no matter where it occurs in the sequence
- In tokenization we can (and typically do) ignore punctuation
- Tokenize之后，计算每个词的出现的次数

#### “Embeddings”
- Each token is mapped to a real-valued **multi-dimensional vector**
    - Vectors loosely represent **semantics** of words
    - Words with similar meanings have vectors that are close (loosely)
    - Widely used in (deep) neural network models
- A sequence of words can be represented as an ordered sequence of vectors (e.g., see recurrent networks in next week’s lecture) 

### Tokenization
- Split up text into individual tokens (words/terms)
- Simplest approach is to ignore all punctuation and white space and use only unbroken strings of alphabetic characters as tokens
- **Many Options and Special Cases in Tokenization**
  - 词性、缩写、连词、特有名词 

#### Vocabulary
- Set of character strings (terms) that a model recognizes
- Other strings are treated as a generic “unknown” term (out of vocabulary)
- **Stopwords**
  - `from nltk.corpus import stopwords`
- **Vocabulary Size**
- **Word Frequency**
  - ![](http://www.prismnet.com/~dierdorf/wordfrequency.png)
- Heuristic 实际处理方法
  - Option 1: remove very common terms (e.g., stop words)
  - Option 2: remove very rare tokens: 
    - Gets rid of **misspellings**, **unusual names**, etc
    - It may be worth trying different values to assess downstream performance
- **“Non-word” symbols** are often useful to encode in the vocabulary
- Proper nouns (people’s names)
- Encoding “sub-words” (e.g., “ing”) as tokens
- These decisions often depend on the nature of the **“downstream”** application

#### Adding N-grams to the Vocabulary
- Useful n-grams are groups of n-words that commonly appear in sequence
  - Computer Sicence
  - Note that we can have both terms like “computer” and “computer science”
- Adding an n-gram explicitly as a term in the vocabulary may be useful 
  - for a restaurant review classification problem, “wait time” may be more informative than “time”


### Machine Learning Procedure

#### Text Classification
- Feature Representations
    - “Bag of words/terms” commonly used: either counts or binary
    - More modern methods use embeddings and/or sequential models
    - Can also use other weighting and additional features (e.g., metadata)
- Classification Methods
    - Naïve Bayes widely used baseline
    - Logistic Classification 
        - Old idea, still widely used in industry: can be an accurate, excellent baseline
    - Neural networks and deep learning
        - Can be very accurate (more flexible than logistic regression)
        - Can take **word sequence** into account (see next week’s lectures)
        - Can require very large amounts of labeled training data
        - More complex than other methods

##### Linear Classifier for Documents
-  Linear classifiers use a weighted sum of the inputs 
    - With `d` features we have `d + 1`  weights (one per feature plus one “intercept”)
- Inner Products and Linear Classifier $$w x = \sum{w_j \cdot x_j}$$
#### Manually-Defined Features
- Add features based on heuristic
- 有没有大小写，有没有连字符

#### Named Entity Recognition (NER)
- Classify a word or phrase as Person, Place, or Organization
- Combinations of approaches
  - Data-driven classification methods (use local context)
  - Dictionary look-up (“lexicons”) for lists of names

